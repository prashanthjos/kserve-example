{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Fraud Detection Inference Service\n",
    "\n",
    "In this notebook, we'll test our deployed fraud detection model with KServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_data = pd.read_csv('data/credit_card_data.csv').sample(10, random_state=42)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the predictor URL (you should get this from the previous notebook)\n",
    "predictor_url = \"http://<your-kserve-endpoint>/v1/models/fraud-detection:predict\"\n",
    "\n",
    "# Prepare test data for prediction\n",
    "feature_columns = [col for col in test_data.columns if col != 'Class']\n",
    "X_test = test_data[feature_columns].values.tolist()\n",
    "\n",
    "# Create prediction payload\n",
    "payload = {\n",
    "    \"instances\": X_test\n",
    "}\n",
    "\n",
    "# Send prediction request\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "response = requests.post(predictor_url, json=payload, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    predictions = response.json()\n",
    "    print(\"Predictions:\")\n",
    "    print(json.dumps(predictions, indent=2))\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the explainer URL\n",
    "explainer_url = \"http://<your-kserve-endpoint>/v1/models/fraud-detection:explain\"\n",
    "\n",
    "# Send explanation request (using the same data)\n",
    "response = requests.post(explainer_url, json=payload, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    explanations = response.json()\n",
    "    print(\"Explanations:\")\n",
    "    print(json.dumps(explanations, indent=2))\n",
    "    \n",
    "    # Visualize explanations for the first instance\n",
    "    if 'explanations' in explanations and len(explanations['explanations']) > 0:\n",
    "        first_explanation = explanations['explanations'][0]\n",
    "        feature_importance = first_explanation['feature_importance']\n",
    "        \n",
    "        # Sort features by importance\n",
    "        sorted_features = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        top_features = sorted_features[:10]  # Top 10 features\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        feature_names = [f[0] for f in top_features]\n",
    "        feature_values = [f[1] for f in top_features]\n",
    "        \n",
    "        # Create colormap based on positive/negative values\n",
    "        colors = ['red' if x < 0 else 'green' for x in feature_values]\n",
    "        \n",
    "        plt.barh(feature_names, feature_values, color=colors)\n",
    "        plt.xlabel('SHAP Value (Impact on Prediction)')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title('Top Features Influencing Fraud Prediction')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Performance Under Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def make_prediction(payload):\n",
    "    start_time = time.time()\n",
    "    response = requests.post(predictor_url, json=payload, headers=headers)\n",
    "    end_time = time.time()\n",
    "    return {\n",
    "        \"status_code\": response.status_code,\n",
    "        \"response_time\": end_time - start_time\n",
    "    }\n",
    "\n",
    "# Generate multiple test instances (100 instances)\n",
    "num_instances = 100\n",
    "large_test_data = pd.read_csv('data/credit_card_data.csv').sample(num_instances, random_state=42)\n",
    "X_large_test = large_test_data[feature_columns].values.tolist()\n",
    "\n",
    "# Split into batches of 10 instances each\n",
    "batch_size = 10\n",
    "batches = [X_large_test[i:i + batch_size] for i in range(0, len(X_large_test), batch_size)]\n",
    "payloads = [{\"instances\": batch} for batch in batches]\n",
    "\n",
    "# Make concurrent requests\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(make_prediction, payload) for payload in payloads]\n",
    "    for future in futures:\n",
    "        results.append(future.result())\n",
    "\n",
    "# Analyze results\n",
    "response_times = [result[\"response_time\"] for result in results]\n",
    "success_count = sum(1 for result in results if result[\"status_code\"] == 200)\n",
    "\n",
    "print(f\"Success Rate: {success_count}/{len(results)} ({success_count/len(results)*100:.2f}%)\")\n",
    "print(f\"Average Response Time: {np.mean(response_times):.4f} seconds\")\n",
    "print(f\"Min Response Time: {np.min(response_times):.4f} seconds\")\n",
    "print(f\"Max Response Time: {np.max(response_times):.4f} seconds\")\n",
    "\n",
    "# Plot response time distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(response_times, kde=True)\n",
    "plt.xlabel('Response Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Response Times')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canary Deployment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we'll test the canary deployment\n",
    "# First, we need to modify the model and deploy a v2 version\n",
    "# This would typically be done in a separate pipeline run\n",
    "\n",
    "import kfp\n",
    "from src.pipeline.pipeline import fraud_detection_pipeline\n",
    "\n",
    "# Compile the pipeline\n",
    "pipeline_func = fraud_detection_pipeline\n",
    "pipeline_filename = \"fraud_detection_pipeline_v2.yaml\"\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "\n",
    "# Connect to the Kubeflow Pipelines API\n",
    "client = kfp.Client()\n",
    "\n",
    "# Run the pipeline for v2 model\n",
    "run_v2 = client.run_pipeline(\n",
    "    experiment_id=experiment.id,\n",
    "    job_name=\"fraud-detection-training-v2\",\n",
    "    pipeline_package_path=pipeline_filename,\n",
    "    params={\n",
    "        \"data_path\": \"data/credit_card_data.csv\",\n",
    "        \"model_name\": \"fraud-detection\",\n",
    "        \"model_version\": \"v2\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Pipeline run for v2 model submitted with ID: {run_v2.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the v2 model is trained, deploy canary\n",
    "from kubernetes import client, config\n",
    "from kserve import KServeClient\n",
    "\n",
    "# Load Kubernetes configuration\n",
    "config.load_kube_config()\n",
    "\n",
    "# Create KServe client\n",
    "kserve_client = KServeClient()\n",
    "\n",
    "# Load the canary deployment YAML\n",
    "with open('kserve/canary_rollout.yaml', 'r') as f:\n",
    "    canary_service = yaml.safe_load(f)\n",
    "\n",
    "# Deploy the canary\n",
    "kserve_client.replace(canary_service)\n",
    "\n",
    "print(f\"Canary deployment started for {service_name} with 20% traffic to v2\")\n",
    "wait_for_service_ready(service_name, namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the canary deployment\n",
    "# We'll make multiple calls and see which version responds\n",
    "num_calls = 50\n",
    "version_counts = {'v1': 0, 'v2': 0, 'unknown': 0}\n",
    "\n",
    "for i in range(num_calls):\n",
    "    response = requests.post(predictor_url, json=payload, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # This is a simplified way to check version - in reality\n",
    "        # you would need to include model version in the response\n",
    "        # or check model-specific differences in predictions\n",
    "        version_counts['unknown'] += 1\n",
    "    else:\n",
    "        print(f\"Error on call {i}: {response.status_code}\")\n",
    "\n",
    "print(\"Calls distribution:\")\n",
    "for version, count in version_counts.items():\n",
    "    print(f\"{version}: {count} ({count/num_calls*100:.2f}%)\")\n",
    "\n",
    "# In a real scenario, you'd want to look at model headers or other\n",
    "# identifiers to track which version of the model is responding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring KServe Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the service metrics\n",
    "from kubernetes import client\n",
    "\n",
    "# Create a custom objects API client\n",
    "api_instance = client.CustomObjectsApi()\n",
    "\n",
    "# Get service metrics\n",
    "try:\n",
    "    metrics = api_instance.get_namespaced_custom_object(\n",
    "        group=\"serving.kubeflow.org\",\n",
    "        version=\"v1beta1\",\n",
    "        namespace=namespace,\n",
    "        plural=\"inferenceservices\",\n",
    "        name=service_name\n",
    "    )\n",
    "    \n",
    "    # Extract status and metrics\n",
    "    status = metrics.get('status', {})\n",
    "    \n",
    "    print(\"Service Status:\")\n",
    "    print(json.dumps(status, indent=2))\n",
    "    \n",
    "    # If using Prometheus, we could also query it directly\n",
    "    # for more detailed metrics\n",
    "except Exception as e:\n",
    "    print(f\"Error getting metrics: {e}\")"
   ]
  }
 ]
}